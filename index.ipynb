{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    Activation,\n",
    "    merge,\n",
    "    Dropout,\n",
    "    Reshape,\n",
    "    Permute,\n",
    "    Dense,\n",
    "    UpSampling2D,\n",
    "    Flatten\n",
    "    )\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.layers.convolutional import (\n",
    "    Convolution2D)\n",
    "from keras.layers.pooling import (\n",
    "    MaxPooling2D,\n",
    "    AveragePooling2D\n",
    "    )\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 1e-5\n",
    "\n",
    "# Convolutional blocks\n",
    "def conv2d_bn_relu(filter_sz, row, col, subsample=(1,1)):\n",
    "    def f(input):\n",
    "        x = Convolution2D(filter_sz, row, col, subsample=subsample, \n",
    "                             init='orthogonal', border_mode='same', bias=False)(input)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(activation='relu')(x)\n",
    "        return x\n",
    "    return f\n",
    "\n",
    "def conv2d_bn_relu_x2(filter_sz, row, col, subsample=(1,1)):\n",
    "    def f(input):\n",
    "        x = Convolution2D(filter_sz, row, col, subsample=subsample, \n",
    "                             init='orthogonal', border_mode='same', bias=False,\n",
    "                             W_regularizer=l2(weight_decay),\n",
    "                             b_regularizer=l2(weight_decay))(input)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(activation='relu')(x)\n",
    "        \n",
    "        x = Convolution2D(filter_sz, row, col, subsample=subsample, \n",
    "                             init='orthogonal', border_mode='same', bias=False,\n",
    "                             W_regularizer=l2(weight_decay),\n",
    "                             b_regularizer=l2(weight_decay))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(activation='relu')(x)\n",
    "        return x\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fcrn_a(input):\n",
    "    \n",
    "    # Block 1\n",
    "    x = conv2d_bn_relu_x2(32,3,3)(input)\n",
    "    x = MaxPooling2D(2,2)(x)\n",
    "    # ==============================\n",
    "    # Block 2\n",
    "    x = conv2d_bn_relu_x2(64,3,3)(x)\n",
    "    x = MaxPooling2D(2,2)(x)\n",
    "    # ==============================\n",
    "    # Block 3\n",
    "    x = conv2d_bn_relu_x2(128,3,3)(x)\n",
    "    x = MaxPooling2D(2,2)(x)\n",
    "    # ==============================\n",
    "    # Block 4\n",
    "    x = conv2d_bn_relu(512,3,3)(x)\n",
    "    # ==============================\n",
    "    # Block 5\n",
    "    x = UpSampling2D(2,2)(x)\n",
    "    x = conv2d_bn_relu_x2(128,3,3)(x)\n",
    "    # ==============================\n",
    "    # Block 6\n",
    "    x = UpSampling2D(2,2)(x)\n",
    "    x = conv2d_bn_relu_x2(64,3,3)(x)\n",
    "    # ==============================\n",
    "    # Block 7\n",
    "    x = UpSampling2D(2,2)(x)\n",
    "    x = conv2d_bn_relu_x2(32,3,3)(x)\n",
    "    # ==============================\n",
    "    \n",
    "    return x\n",
    "\n",
    "def Unet(input,filter_sz=64):\n",
    "    \n",
    "    block1 = conv2d_bn_relu_x2(filter_sz,3,3)(input)\n",
    "    pool1 = MaxPooling2D(2,2)(block1)\n",
    "    # ==============================\n",
    "    block2 = conv2d_bn_relu_x2(filter_sz,3,3)(pool1)\n",
    "    pool2 = MaxPooling2D(2,2)(block2)\n",
    "    # ==============================\n",
    "    block3 = conv2d_bn_relu_x2(filter_sz,3,3)(pool2)\n",
    "    pool3 = MaxPooling2D(2,2)(block3)\n",
    "    # ==============================\n",
    "    block4 = conv2d_bn_relu_x2(filter_sz,3,3)(pool3)\n",
    "    up4 = merge([UpSampling2D(size=(2, 2))(block4), block3], mode='concat', concat_axis=-1)\n",
    "    # ==============================\n",
    "    block5 = conv2d_bn_relu_x2(filter_sz,3,3)(up4)\n",
    "    up5 = merge([UpSampling2D(size=(2, 2))(block5), block2], mode='concat', concat_axis=-1)\n",
    "    # ==============================\n",
    "    block6 = conv2d_bn_relu_x2(filter_sz,3,3)(up5)\n",
    "    up6 = merge([UpSampling2D(size=(2, 2))(block6), block1], mode='concat', concat_axis=-1)\n",
    "    # ==============================\n",
    "    block7 = conv2d_bn_relu_x2(filter_sz,3,3)(up6)\n",
    "    # ==============================\n",
    "    \n",
    "    return block7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildFcrn_a(input_dim):\n",
    "    \n",
    "    input_ = Input(input_shape=(input_dim))\n",
    "    base = Fcrn_a(input_)\n",
    "    density_pred =  Convolution2D(1, 1, 1, bias = False, activation='linear',\\\n",
    "                                  init='orthogonal',name='pred',border_mode='same')(base)\n",
    "    \n",
    "    model = Model(input=input_, output=density_pred)\n",
    "    opt = SGD(lr = 1e-2, momentum = 0.9, nesterov = True)\n",
    "    model.compile(optimizer=opt, loss='mse')\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def BuildUnet():\n",
    "    input_ = Input (shape = (input_dim))\n",
    "    base = U_net_base (input_, nb_filter = 64 )\n",
    "    density_pred =  Convolution2D(1, 1, 1, bias = False, activation='linear',\\\n",
    "                                  init='orthogonal',name='pred',border_mode='same')(base)\n",
    "    \n",
    "    model = Model (input=input_, output=density_pred)\n",
    "    opt = RMSprop(1e-3)\n",
    "    model.compile(optimizer = opt, loss = 'mse')\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wget'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-b72b22a162d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclick\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wget'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "from glob import glob\n",
    "from typing import List, Tuple\n",
    "\n",
    "import click\n",
    "import h5py\n",
    "import wget\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.io import loadmat\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "\n",
    "def get_data(dataset: str):\n",
    "    \"\"\"\n",
    "    Get chosen dataset and generate HDF5 files with training\n",
    "    and validation samples.\n",
    "    \"\"\"\n",
    "    # dictionary-based switch statement\n",
    "    {\n",
    "        'cell': generate_cell_data,\n",
    "        'mall': generate_mall_data,\n",
    "        'ucsd': generate_ucsd_data\n",
    "    }[dataset]()\n",
    "\n",
    "\n",
    "def create_hdf5(dataset_name: str,\n",
    "                train_size: int,\n",
    "                valid_size: int,\n",
    "                img_size: Tuple[int, int],\n",
    "                in_channels: int=3):\n",
    "    \"\"\"\n",
    "    Create empty training and validation HDF5 files with placeholders\n",
    "    for images and labels (density maps).\n",
    "    Note:\n",
    "    Datasets are saved in [dataset_name]/train.h5 and [dataset_name]/valid.h5.\n",
    "    Existing files will be overwritten.\n",
    "    Args:\n",
    "        dataset_name: used to create a folder for train.h5 and valid.h5\n",
    "        train_size: no. of training samples\n",
    "        valid_size: no. of validation samples\n",
    "        img_size: (width, height) of a single image / density map\n",
    "        in_channels: no. of channels of an input image\n",
    "    Returns:\n",
    "        A tuple of pointers to training and validation HDF5 files.\n",
    "    \"\"\"\n",
    "    # create output folder if it does not exist\n",
    "    os.makedirs(dataset_name, exist_ok=True)\n",
    "\n",
    "    # create HDF5 files: [dataset_name]/(train | valid).h5\n",
    "    train_h5 = h5py.File(os.path.join(dataset_name, 'train.h5'), 'w')\n",
    "    valid_h5 = h5py.File(os.path.join(dataset_name, 'valid.h5'), 'w')\n",
    "\n",
    "    # add two HDF5 datasets (images and labels) for each HDF5 file\n",
    "    for h5, size in ((train_h5, train_size), (valid_h5, valid_size)):\n",
    "        h5.create_dataset('images', (size, in_channels, *img_size))\n",
    "        h5.create_dataset('labels', (size, 1, *img_size))\n",
    "\n",
    "    return train_h5, valid_h5\n",
    "\n",
    "\n",
    "def generate_label(label_info: np.array, image_shape: List[int]):\n",
    "    \"\"\"\n",
    "    Generate a density map based on objects positions.\n",
    "    Args:\n",
    "        label_info: (x, y) objects positions\n",
    "        image_shape: (width, height) of a density map to be generated\n",
    "    Returns:\n",
    "        A density map.\n",
    "    \"\"\"\n",
    "    # create an empty density map\n",
    "    label = np.zeros(image_shape, dtype=np.float32)\n",
    "\n",
    "    # loop over objects positions and marked them with 100 on a label\n",
    "    # note: *_ because some datasets contain more info except x, y coordinates\n",
    "    for x, y, *_ in label_info:\n",
    "        if y < image_shape[0] and x < image_shape[1]:\n",
    "            label[int(y)][int(x)] = 100\n",
    "\n",
    "    # apply a convolution with a Gaussian kernel\n",
    "    label = gaussian_filter(label, sigma=(1, 1), order=0)\n",
    "\n",
    "    return label\n",
    "\n",
    "\n",
    "def get_and_unzip(url: str, location: str=\".\"):\n",
    "    \"\"\"Extract a ZIP archive from given URL.\n",
    "    Args:\n",
    "        url: url of a ZIP file\n",
    "        location: target location to extract archive in\n",
    "    \"\"\"\n",
    "    dataset = wget.download(url)\n",
    "    dataset = zipfile.ZipFile(dataset)\n",
    "    dataset.extractall(location)\n",
    "    dataset.close()\n",
    "    os.remove(dataset.filename)\n",
    "    \n",
    "\n",
    "def generate_ucsd_data():\n",
    "    \"\"\"Generate HDF5 files for mall dataset.\"\"\"\n",
    "    # download and extract data\n",
    "    get_and_unzip(\n",
    "        'http://www.svcl.ucsd.edu/projects/peoplecnt/db/ucsdpeds.zip'\n",
    "    )\n",
    "    # download and extract annotations\n",
    "    get_and_unzip(\n",
    "        'http://www.svcl.ucsd.edu/projects/peoplecnt/db/vidf-cvpr.zip'\n",
    "    )\n",
    "    # create training and validation HDF5 files\n",
    "    train_h5, valid_h5 = create_hdf5('ucsd',\n",
    "                                     train_size=1500,\n",
    "                                     valid_size=500,\n",
    "                                     img_size=(160, 240),\n",
    "                                     in_channels=1)\n",
    "\n",
    "    def fill_h5(h5, labels, video_id, init_frame=0, h5_id=0):\n",
    "        \"\"\"\n",
    "        Save images and labels in given HDF5 file.\n",
    "        Args:\n",
    "            h5: HDF5 file\n",
    "            labels: the list of labels\n",
    "            video_id: the id of a scene\n",
    "            init_frame: the first frame in given list of labels\n",
    "            h5_id: next dataset id to be used\n",
    "        \"\"\"\n",
    "        video_name = f\"vidf1_33_00{video_id}\"\n",
    "        video_path = f\"ucsdpeds/vidf/{video_name}.y/\"\n",
    "\n",
    "        for i, label in enumerate(labels, init_frame):\n",
    "            # path to the next frame (convention: [video name]_fXXX.jpg)\n",
    "            img_path = f\"{video_path}/{video_name}_f{str(i+1).zfill(3)}.png\"\n",
    "\n",
    "            # get an image as numpy array\n",
    "            image = np.array(Image.open(img_path), dtype=np.float32) / 255\n",
    "            # generate a density map by applying a Gaussian filter\n",
    "            label = generate_label(label[0][0][0], image.shape)\n",
    "\n",
    "            # pad images to allow down and upsampling\n",
    "            image = np.pad(image, 1, 'constant', constant_values=0)\n",
    "            label = np.pad(label, 1, 'constant', constant_values=0)\n",
    "\n",
    "            # save data to HDF5 file\n",
    "            h5['images'][h5_id + i - init_frame, 0] = image\n",
    "            h5['labels'][h5_id + i - init_frame, 0] = label\n",
    "\n",
    "    # dataset contains 10 scenes\n",
    "    for scene in range(10):\n",
    "        # load labels infomation from provided MATLAB file\n",
    "        # it is numpy array with (x, y) objects position for subsequent frames\n",
    "        descriptions = loadmat(f'vidf-cvpr/vidf1_33_00{scene}_frame_full.mat')\n",
    "        labels = descriptions['frame'][0]\n",
    "\n",
    "        # use first 150 frames for training and the last 50 for validation\n",
    "        # start filling from the place last scene finished\n",
    "        fill_h5(train_h5, labels[:150], scene, 0, 150 * scene)\n",
    "        fill_h5(valid_h5, labels[150:], scene, 150, 50 * scene)\n",
    "\n",
    "    # close HDF5 files\n",
    "    train_h5.close()\n",
    "    valid_h5.close()\n",
    "\n",
    "    # cleanup\n",
    "    shutil.rmtree('ucsdpeds')\n",
    "    shutil.rmtree('vidf-cvpr')\n",
    "\n",
    "\n",
    "def generate_mall_data():\n",
    "    \"\"\"Generate HDF5 files for mall dataset.\"\"\"\n",
    "    # download and extract dataset\n",
    "    get_and_unzip(\n",
    "        'http://personal.ie.cuhk.edu.hk/~ccloy/files/datasets/mall_dataset.zip'\n",
    "    )\n",
    "    # create training and validation HDF5 files\n",
    "    train_h5, valid_h5 = create_hdf5('mall',\n",
    "                                     train_size=1500,\n",
    "                                     valid_size=500,\n",
    "                                     img_size=(480, 640),\n",
    "                                     in_channels=3)\n",
    "\n",
    "    # load labels infomation from provided MATLAB file\n",
    "    # it is a numpy array with (x, y) objects position for subsequent frames\n",
    "    labels = loadmat('mall_dataset/mall_gt.mat')['frame'][0]\n",
    "\n",
    "    def fill_h5(h5, labels, init_frame=0):\n",
    "        \"\"\"\n",
    "        Save images and labels in given HDF5 file.\n",
    "        Args:\n",
    "            h5: HDF5 file\n",
    "            labels: the list of labels\n",
    "            init_frame: the first frame in given list of labels\n",
    "        \"\"\"\n",
    "        for i, label in enumerate(labels, init_frame):\n",
    "            # path to the next frame (filename convention: seq_XXXXXX.jpg)\n",
    "            img_path = f\"mall_dataset/frames/seq_{str(i+1).zfill(6)}.jpg\"\n",
    "\n",
    "            # get an image as numpy array\n",
    "            image = np.array(Image.open(img_path), dtype=np.float32) / 255\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "\n",
    "            # generate a density map by applying a Gaussian filter\n",
    "            label = generate_label(label[0][0][0], image.shape[1:])\n",
    "\n",
    "            # save data to HDF5 file\n",
    "            h5['images'][i - init_frame] = image\n",
    "            h5['labels'][i - init_frame, 0] = label\n",
    "\n",
    "    # use first 1500 frames for training and the last 500 for validation\n",
    "    fill_h5(train_h5, labels[:1500])\n",
    "    fill_h5(valid_h5, labels[1500:], 1500)\n",
    "\n",
    "    # close HDF5 file\n",
    "    train_h5.close()\n",
    "    valid_h5.close()\n",
    "\n",
    "    # cleanup\n",
    "    shutil.rmtree('mall_dataset')\n",
    "\n",
    "\n",
    "def generate_cell_data():\n",
    "    \"\"\"Generate HDF5 files for fluorescent cell dataset.\"\"\"\n",
    "    # download and extract dataset\n",
    "    get_and_unzip(\n",
    "        'http://www.robots.ox.ac.uk/~vgg/research/counting/cells.zip',\n",
    "        location='cells'\n",
    "    )\n",
    "    # create training and validation HDF5 files\n",
    "    train_h5, valid_h5 = create_hdf5('cell',\n",
    "                                     train_size=150,\n",
    "                                     valid_size=50,\n",
    "                                     img_size=(256, 256),\n",
    "                                     in_channels=3)\n",
    "\n",
    "    # get the list of all samples\n",
    "    # dataset name convention: XXXcell.png (image) XXXdots.png (label)\n",
    "    image_list = glob(os.path.join('cells', '*cell.*'))\n",
    "    image_list.sort()\n",
    "\n",
    "    def fill_h5(h5, images):\n",
    "        \"\"\"\n",
    "        Save images and labels in given HDF5 file.\n",
    "        Args:\n",
    "            h5: HDF5 file\n",
    "            images: the list of images paths\n",
    "        \"\"\"\n",
    "        for i, img_path in enumerate(images):\n",
    "            # get label path\n",
    "            label_path = img_path.replace('cell.png', 'dots.png')\n",
    "            # get an image as numpy array\n",
    "            image = np.array(Image.open(img_path), dtype=np.float32) / 255\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "\n",
    "            # convert a label image into a density map: dataset provides labels\n",
    "            # in the form on an image with red dots placed in objects position\n",
    "\n",
    "            # load an RGB image\n",
    "            label = np.array(Image.open(label_path))\n",
    "            # make a one-channel label array with 100 in red dots positions\n",
    "            label = 100.0 * (label[:, :, 0] > 0)\n",
    "            # generate a density map by applying a Gaussian filter\n",
    "            label = gaussian_filter(label, sigma=(1, 1), order=0)\n",
    "\n",
    "            # save data to HDF5 file\n",
    "            h5['images'][i] = image\n",
    "            h5['labels'][i, 0] = label\n",
    "\n",
    "    # use first 150 samples for training and the last 50 for validation\n",
    "    fill_h5(train_h5, image_list[:150])\n",
    "    fill_h5(valid_h5, image_list[150:])\n",
    "\n",
    "    # close HDF5 files\n",
    "    train_h5.close()\n",
    "    valid_h5.close()\n",
    "\n",
    "    # cleanup\n",
    "    shutil.rmtree('cells')\n",
    "\n",
    "generate_cell_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
